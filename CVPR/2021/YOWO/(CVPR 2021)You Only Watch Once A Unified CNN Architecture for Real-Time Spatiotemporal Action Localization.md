# (CVPR 2021)You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization

## 1. 动机

**时空视频的动作定位(spatiotemporal human action localization)**不仅需要在空间上定位人体动作在空间上的位置，还需要在时序上定位动作的起始，并对该动作进行分类。

相比静态的目标检测，时序信息对时空视频的动作定位十分重要，对于单帧图片能够成功地定位人体的位置，但是该人体的动作需要结合过去帧的信息才能正确判断。（*例如站起和坐下的区分*）因此==**如何结合时序和空间信息十分重要**==。

![fig_1](.\img\fig_1.png)

受Faster RCNN网络框架的启发，大部分已有方法都属于两阶段方法，存在一些不足：

1. action tube的生成需要对不同帧生成大量的候选区域，相比2D情况下更加的复杂和耗时。

   > action tube 是对每一帧中检测出的bounding box进行链接得到的

2. 网络对人体动作的分类将基于生成的候选区域进行分类，候选区域的质量将影响分类的性能，同时仅依赖候选区域中的人体特征，忽视了**人与人或物交互的信息及一些背景信息**。这些上下文信息对动作识别十分重要。

3. 分别训练RPN网络及分类网络无法保证得到全局最优的模型。

   > 分别训练时，RPN网络和分类网络都能达到各自的全局最优，但是组合起来得到的最终模型并不一定是最优的。

4. 相比一阶段方法，两阶段方法的训练时长较长，需要更多内存。

### 主要贡献

作者提出YOWO的一阶段方法来避免上述二阶段方法的缺点，同时使用不同的输入分支提取时序和空间信息。添加不同的输入分支能够为网络提供其他不同模特的数据。

其中YOWO从历史帧中提取时序信息。为了更快的运行速度，YOWO最多处理16帧的输入，然而如此短的clip输入提供的时序信息是有限的，而更长的输入会减慢网络的运行速度。

> 对于视频中不同帧的预测可能会对同一段视频帧进行重复处理？

## 2. 方法

### YOWO 网络结构

